test.error <- mape(yhat.test, hmwk.test$emp)
yhat <- c(yhat.train, yhat.test)
# append to model performance dataframe
df_performance <- data.frame(hmwk.temp$GEOID, r.squared,
train.error,test.error)
model.performance <- rbind(model.performance, unique(df_performance))
# append to model prediction dataframe
df_prediction <- data.frame(hmwk.temp$GEOID, hmwk.temp$date,
yhat)
model.predictions <- rbind(model.predictions, df_prediction)
}
# rename the columns
colnames(model.performance) <- c("GEOID", "r.squared","train.error","test.error")
colnames(model.predictions) <- c("GEOID", "date", 'yhat')
# create variable rank that maximize R-squared and minimize test errors
model.performance.4 <- model.performance
model.performance.4$rank <- model.performance.4$r.squared/model.performance.4$test.error
# sort model.performance based on rank
model.performance.4 <- model.performance.4[order(-model.performance.4$rank), ]
# get top 10 metro areas
top.10.list4 <- model.performance.4[1:10, c("GEOID", "rank")]
top.10.list4
model.performance4
model.performance.4
temp <- tempfile()
download.file("https://github.com/SigmaMonstR/rudy.js/raw/master/temp.Rda", temp, mode="wb")
load(temp)
#Utilities to help you
mape <- function(yhat, y){
#yhat = your prediction
#y = original value
#Note that code will ignore any missing values
return(100*mean(abs(yhat/y - 1), na.rm=T))
}
#Write your code and annotation below
#Note: Do not delete records with missing values (particularly in emp),
#As those records are ones you'll predict
# see the patterns of the data
str(hmwk)
# data cleaning
hmwk$GEOID <- as.numeric(hmwk$GEOID)
# see how many missing values in data for each features
sapply(hmwk, function(x){sum(is.na(x))})
# create useable dataset(have complete emp records)
hmwk.use <- hmwk[hmwk$complete == "TRUE", ]
# leave aside remaining incomplete emp data for score
hmwk.remain <- hmwk[hmwk$complete == "FALSE", ]
model.performance <- data.frame()
model.predictions <- data.frame()
yhat <- vector()
myPredictions <- data.frame()
for (i in unique(hmwk.use$GEOID)) {
hmwk.temp <- hmwk.use[hmwk.use$GEOID == i, ]
# create training set
hmwk.train <- hmwk.temp[1:floor(0.7*nrow(hmwk.temp)), ]
# create test set
hmwk.test <- hmwk.temp[ceiling(0.7*nrow(hmwk.temp)):nrow(hmwk.temp), ]
# OLS model
fit <- lm(emp ~ avg_rad.mean + year, data = hmwk.train)
r.squared <- summary(fit)$r.squared
# calculate train predicted values & error
# calculate test predicted values & error
yhat.train <- predict(fit, newdata = hmwk.train)
train.error <- mape(yhat.train, hmwk.train$emp)
yhat.test <- predict(fit, newdata = hmwk.test)
test.error <- mape(yhat.test, hmwk.test$emp)
yhat <- c(yhat.train, yhat.test)
# append to model performance dataframe
df.performance <- data.frame(hmwk.temp$GEOID, r.squared,
train.error,test.error)
model.performance <- rbind(model.performance, unique(df.performance))
# append to model prediction dataframe
df.prediction <- data.frame(hmwk.temp$GEOID, hmwk.temp$date,
yhat)
model.predictions <- rbind(model.predictions, df.prediction)
}
# rename the columns
colnames(model.performance) <- c("GEOID", "r.squared","train.error","test.error")
colnames(model.predictions) <- c("GEOID", "date", 'yhat')
# create variable rank that maximize R-squared and minimize test errors
model.performance$rank <- model.performance$r.squared/model.performance$test.error
# sort model.performance based on rank
model.performance <- model.performance[order(-model.performance$rank), ]
# get top 10 metro areas
top.10.list4 <- model.performance[1:10, c("GEOID", "rank")]
# populate myPredictioins dataframe
myPredictions <- model.predictions[model.predictions$GEOID %in% top.10.list, ]
temp <- tempfile()
download.file("https://github.com/SigmaMonstR/rudy.js/raw/master/temp.Rda", temp, mode="wb")
load(temp)
#Utilities to help you
mape <- function(yhat, y){
#yhat = your prediction
#y = original value
#Note that code will ignore any missing values
return(100*mean(abs(yhat/y - 1), na.rm=T))
}
#Write your code and annotation below
#Note: Do not delete records with missing values (particularly in emp),
#As those records are ones you'll predict
# see the patterns of the data
str(hmwk)
# data cleaning
hmwk$GEOID <- as.numeric(hmwk$GEOID)
# see how many missing values in data for each features
sapply(hmwk, function(x){sum(is.na(x))})
# create useable dataset(have complete emp records)
hmwk.use <- hmwk[hmwk$complete == "TRUE", ]
# leave aside remaining incomplete emp data for score
hmwk.remain <- hmwk[hmwk$complete == "FALSE", ]
model.performance <- data.frame()
model.predictions <- data.frame()
yhat <- vector()
myPredictions <- data.frame()
for (i in unique(hmwk.use$GEOID)) {
hmwk.temp <- hmwk.use[hmwk.use$GEOID == i, ]
# create training set
hmwk.train <- hmwk.temp[1:floor(0.7*nrow(hmwk.temp)), ]
# create test set
hmwk.test <- hmwk.temp[ceiling(0.7*nrow(hmwk.temp)):nrow(hmwk.temp), ]
# OLS model
fit <- lm(emp ~ avg_rad.mean + year, data = hmwk.train)
r.squared <- summary(fit)$r.squared
# calculate train predicted values & error
# calculate test predicted values & error
yhat.train <- predict(fit, newdata = hmwk.train)
train.error <- mape(yhat.train, hmwk.train$emp)
yhat.test <- predict(fit, newdata = hmwk.test)
test.error <- mape(yhat.test, hmwk.test$emp)
yhat <- c(yhat.train, yhat.test)
# append to model performance dataframe
df.performance <- data.frame(hmwk.temp$GEOID, r.squared,
train.error,test.error)
model.performance <- rbind(model.performance, unique(df.performance))
# append to model prediction dataframe
df.prediction <- data.frame(hmwk.temp$GEOID, hmwk.temp$date,
yhat)
model.predictions <- rbind(model.predictions, df.prediction)
}
# rename the columns
colnames(model.performance) <- c("GEOID", "r.squared","train.error","test.error")
colnames(model.predictions) <- c("GEOID", "date", 'yhat')
# create variable rank that maximize R-squared and minimize test errors
model.performance$rank <- model.performance$r.squared/model.performance$test.error
# sort model.performance based on rank
model.performance <- model.performance[order(-model.performance$rank), ]
# get top 10 metro areas
top.10.list <- model.performance[1:10, c("GEOID", "rank")]
# populate myPredictioins dataframe
myPredictions <- model.predictions[model.predictions$GEOID %in% top.10.list, ]
head(myPredictions)
top.10.list == top.10.list4
head(model.predictions)
model.performance <- data.frame()
model.predictions <- data.frame()
yhat <- vector()
myPredictions <- data.frame()
for (i in unique(hmwk.use$GEOID)) {
hmwk.temp <- hmwk.use[hmwk.use$GEOID == i, ]
# create training set
hmwk.train <- hmwk.temp[1:floor(0.7*nrow(hmwk.temp)), ]
# create test set
hmwk.test <- hmwk.temp[ceiling(0.7*nrow(hmwk.temp)):nrow(hmwk.temp), ]
# OLS model
fit <- lm(emp ~ avg_rad.mean + year, data = hmwk.train)
r.squared <- summary(fit)$r.squared
# calculate train predicted values & error
# calculate test predicted values & error
yhat.train <- predict(fit, newdata = hmwk.train)
train.error <- mape(yhat.train, hmwk.train$emp)
yhat.test <- predict(fit, newdata = hmwk.test)
test.error <- mape(yhat.test, hmwk.test$emp)
yhat <- c(yhat.train, yhat.test)
# append to model performance dataframe
df.performance <- data.frame(hmwk.temp$GEOID, r.squared,
train.error,test.error)
model.performance <- rbind(model.performance, unique(df.performance))
# append to model prediction dataframe
df.prediction <- data.frame(hmwk.temp$GEOID, hmwk.temp$date,
yhat)
model.predictions <- rbind(model.predictions, df.prediction)
}
# rename the columns
colnames(model.performance) <- c("GEOID", "r.squared","train.error","test.error")
colnames(model.predictions) <- c("GEOID", "date", 'yhat')
# create variable rank that maximize R-squared and minimize test errors
model.performance$rank <- model.performance$r.squared/model.performance$test.error
# sort model.performance based on rank
model.performance <- model.performance[order(-model.performance$rank), ]
# get top 10 metro areas
top.10.list <- model.performance[1:10, 1]
# populate myPredictioins dataframe
myPredictions <- model.predictions[model.predictions$GEOID %in% top.10.list, ]
matrix(rnorm(6),2,3)
mt <- matrix(rnorm(6),2,3)
mt
mt
mt[1]
mt[2]
mt[3]
mt[1:3]
class(mt)
rm(list = ls())
?runif
runif(5,0,1)
install.packages(c("rpart","rpart.plot", "devtools", "gridExtra", "randomForest"))
devtools::install_github("sachsmc/plotROC")
library(devtools)
install_github("1chiffon/REmap")
install_github('lchiffon/REmap')
REmap::chinaIphone
set.seed(125)
out = remap(demoC, title = "REmap", subtitle = "theme:Dark")
out = REmap(demoC, title = "REmap", subtitle = "theme:Dark")
library(REmap)
out = REmap(demoC, title = "REmap", subtitle = "theme:Dark")
out = remap(demoC, title = "REmap", subtitle = "theme:Dark")
plot(out)
demoC
plot(out)
rm(list = ls())
setwd("/Users/yuanxiang/Documents/Project/global-terrorism/data")
load("/Users/yuanxiang/Documents/Project/global-terrorism/data/globalterror12_15.RData")
project_data <- globalterror
new <- c("eventid","country_txt", "latitude","longitude","attacktype1", "targtype1", "targsubtype1", "weaptype1")
new1 <- c("eventid","country_txt", "latitude","longitude","attacktype1", "targtype1")
new2 <- c("eventid","country_txt", "latitude","longitude","weaptype1", "targsubtype1")
tocluster.data <- project_data[new]
tocluster.data <- tocluster.data[tocluster.data$country_txt == "Iraq",]
tocluster.data[, c(3, 4)] <- lapply(tocluster.data[, c(3, 4)], as.numeric)
tocluster.data[, 5:ncol(tocluster.data)] <- lapply(tocluster.data[, 5:ncol(tocluster.data)], as.factor)
library(cluster)
gower.data <- tocluster.data
gowerd <- daisy(gower.data[,5:6], metric = "gower")
gmat <- as.matrix(gowerd)
hc <- hclust(gowerd, method="ward.D")
plot(hc)
gower.data[
which(gmat == max(gmat[gmat != min(gmat)]),
arr.ind = TRUE)[1, ], ]
rect.hclust(hc, k=5, border="red")
groups <- cutree(hc, k=5)
rect.hclust(hc, k=5, border="red")
library(maps)
map("world",interior=FALSE,xlim = c(25,91), ylim = c(1,38))
map("world",boundary=FALSE,col="gray",add=TRUE,xlim = c(25,91), ylim = c(1,38))
points(gower.data$longitude,gower.data$latitude,cex=0.1,col=factor(groups))
gower.data$groups <- groups
group <- data.frame()
for (i in 1:5){
group <- gower.data[gower.data$groups == i,]
print(table(group$attacktype1,group$targtype1))
}
rm(list = ls())
set.seed(123)
combo <-  paste(letters[sample(1:26,1000, replace = T)],
LETTERS[sample(1:26,1000, replace = T)],
c(rep(NA,500),c(0:9)[sample(1:10,500, replace = T)]), sep = "|")
head(combo)
str(combo)
?split
strsplit(combo)
strsplit(combo, "|")
str(strsplit(combo, "|"))
str(strsplit(combo, "|"))[1]
strsplit(combo, "|")[[1]]
head(combo)
strsplit(combo, "|")[[1]][[1]]
place <- vector()
for (i in 1:nrow(combo)){
place <- unique(c(strsplit(combo, "|")[[i]][[1]],strsplit(combo, "|")[[i]][[3]], strsplit(combo, "|")[[i]][[5]])  )
}
place <- vector()
for (i in 1:length(combo)){
place <- unique(c(strsplit(combo, "|")[[i]][[1]],strsplit(combo, "|")[[i]][[3]], strsplit(combo, "|")[[i]][[5]])  )
}
strsplit(combo, "|")[[1]][[1]]
strsplit(combo, "|")[[1]][[3]]
place <- vector()
for (i in 1:length(combo)){
place <- unique(c(place, strsplit(combo, "|")[[i]][[1]],strsplit(combo, "|")[[i]][[3]], strsplit(combo, "|")[[i]][[5]])  )
}
data.frame(do.call("rbind", strsplit(combo, "\\|")))
model.matrix(- 0 + V1, mat.or.vec())
mat <- data.frame(do.call("rbind", strsplit(combo, "\\|")))
out <- cbind(model.matrix(~ 0 + V1, mat), model.matrix(~ 0 + V2, mat), model.matrix(~ 0 + V3, mat))
out <- cbind(model.matrix(~ 0 + v1, mat), model.matrix(~ 0 + v2, mat), model.matrix(~ 0 + v3, mat))
out <- data.frame()
for(x in 1:1000){
out <- rbind(out, data.frame(id = x, res = runif(x)))
}
for(x in 1:100){
out <- rbind(out, data.frame(id = x, res = runif(x)))
}
out
head(out)
start <- proc.time()[3]
out <- lapply(1:1000, function(x){ data.frame(id = x, res = runif(x))})
out <- do.call("rbind", out)
timed <- proc.time()[3] - start
timed
class(out)
?do.call
out <- lapply(1:1000, function(x){ data.frame(id = x, res = runif(x))})
out <- do.call("rbind", out)
install.packages("doParallel")
install.packages("foreach")
library(doParallel)
library(foreach)
cl <- makeCluster(2)
detectCores()
registerDoParallel(cl)
cl <- makeCluster(2)
registerDoParallel(cl)
start <- proc.time()[3]
out <- foreach( x = 1:1000, .combine = rbind) %dopar% {
temp <- data.frame(id = x, res = runif(x))
return(temp)
}
timed <- proc.time()[3] - start
?foreach
head(out)
nasaGIBS <- function(inputs){
# Args:
#     inputs should include:
#       timeindex = time index in YYYYJJJ (Y = year, J is day of the year)
#       extent = list of values of bounding box
#       dims =  dimensions of width
#
require(jpeg)
url <- paste0("https://gibs.earthdata.nasa.gov/image-download?TIME=",
inputs$timeindex,
"&extent=", paste(inputs$left, inputs$bottom, inputs$right, inputs$upper,sep=","),
"&epsg=4326&layers=VIIRS_SNPP_CorrectedReflectance_TrueColor&opacities=1&worldfile=false&format=image/jpeg&",
"width=",inputs$width,"&height=", round(inputs$width * abs(inputs$upper - inputs$bottom)/ abs(inputs$left - inputs$right) ))
temp <- tempfile()
download.file(url,  temp, mode = "wb")
return(temp)
}
inputs = list(timeindex = 2017002,
left = -83.81310347041976,
bottom = 31.625676874986674,
right = -72.47521284541976,
upper = 40.133489374986674,
width = 400)
example_out <- nasaGIBS(inputs)
trans <- t(as.vector(example_out))
library(grid)
grid.raster(example_out)
nasaGIBS <- function(inputs){
# Args:
#     inputs should include:
#       timeindex = time index in YYYYJJJ (Y = year, J is day of the year)
#       extent = list of values of bounding box
#       dims =  dimensions of width
#
require(jpeg)
url <- paste0("https://gibs.earthdata.nasa.gov/image-download?TIME=",
inputs$timeindex,
"&extent=", paste(inputs$left, inputs$bottom, inputs$right, inputs$upper,sep=","),
"&epsg=4326&layers=VIIRS_SNPP_CorrectedReflectance_TrueColor&opacities=1&worldfile=false&format=image/jpeg&",
"width=",inputs$width,"&height=", round(inputs$width * abs(inputs$upper - inputs$bottom)/ abs(inputs$left - inputs$right) ))
temp <- tempfile()
download.file(url,  temp, mode = "wb")
return(readJPEG(temp))
}
inputs = list(timeindex = 2017002,
left = -83.81310347041976,
bottom = 31.625676874986674,
right = -72.47521284541976,
upper = 40.133489374986674,
width = 400)
example_out <- nasaGIBS(inputs)
trans <- t(as.vector(example_out))
install.packages("jpeg")
nasaGIBS <- function(inputs){
# Args:
#     inputs should include:
#       timeindex = time index in YYYYJJJ (Y = year, J is day of the year)
#       extent = list of values of bounding box
#       dims =  dimensions of width
#
require(jpeg)
url <- paste0("https://gibs.earthdata.nasa.gov/image-download?TIME=",
inputs$timeindex,
"&extent=", paste(inputs$left, inputs$bottom, inputs$right, inputs$upper,sep=","),
"&epsg=4326&layers=VIIRS_SNPP_CorrectedReflectance_TrueColor&opacities=1&worldfile=false&format=image/jpeg&",
"width=",inputs$width,"&height=", round(inputs$width * abs(inputs$upper - inputs$bottom)/ abs(inputs$left - inputs$right) ))
temp <- tempfile()
download.file(url,  temp, mode = "wb")
return(readJPEG(temp))
}
#How to use Example
inputs = list(timeindex = 2017002,
left = -83.81310347041976,
bottom = 31.625676874986674,
right = -72.47521284541976,
upper = 40.133489374986674,
width = 400)
example_out <- nasaGIBS(inputs)
trans <- t(as.vector(example_out))
trans <- t(as.vector(example_out))
library(grid)
grid.raster(example_out)
dates <- expand.grid(year = seq(2013,2016,1),
month = seq(1,12,1),
day = c(1, 14))
date <- paste0(dates[,c(1,2,3)])
example_list <- paste0(2017,sprintf("%03d", seq(1,70,7)))
start <- proc.time()[3]
test <- lapply(x = example_out, function(x){
inputs = list(timeindex = 2017002,
left = -83.81310347041976,
bottom = 31.625676874986674,
right = -72.47521284541976,
upper = 40.133489374986674,
width = 400)
example_out <- nasaGIBS(inputs)
trans <- t(as.vector(example_out))
return(trans)
})
test2 <- do.call("rbind",test)
proc.time()[3] - start
start <- proc.time()[3]
test <- lapply(x = example_list, function(x){
inputs = list(timeindex = 2017002,
left = -83.81310347041976,
bottom = 31.625676874986674,
right = -72.47521284541976,
upper = 40.133489374986674,
width = 400)
example_out <- nasaGIBS(inputs)
trans <- t(as.vector(example_out))
return(trans)
})
test2 <- do.call("rbind",test)
proc.time()[3] - start
start <- proc.time()[3]
test <- lapply(example_list, function(x){
inputs = list(timeindex = x,
left = -83.81310347041976,
bottom = 31.625676874986674,
right = -72.47521284541976,
upper = 40.133489374986674,
width = 400)
example_out <- nasaGIBS(inputs)
trans <- t(as.vector(example_out))
return(trans)
})
test2 <- do.call("rbind",test)
proc.time()[3] - start
library(foreach)
library(doParallel)
test <- foreach(x = example_list, .combine = rbind) %dopar% {
inputs <- list(timeindex = x,
left = -83.81310347041976,
bottom = 31.625676874986674,
right = -72.47521284541976,
upper = 40.133489374986674,
width = 400)
example_out <- nasaGIBS(inputs)
trans <- t(as.vector(example_out))
return(trans)
}
test2 <- do.call("rbind",test)
proc.time()[3] - start
stopCluster(cl)
?do.call
rm(list = ls())
set.seed(123)
combo <-  paste(letters[sample(1:26,10000, replace = T)],
LETTERS[sample(1:26,10000, replace = T)],
c(rep(NA,5000),c(0:9)[sample(1:10,5000, replace = T)]), sep = "|")
mat <- do.call("rbind",strsplit(combo, "\\|"))
View(mat)
mat <- as.data.frame(mat)
head(mat)
?model.matrix
head(t(mat))
dd <- data.frame(a = gl(3,4), b = gl(4,1,12)) # balanced 2-way
options("contrasts")
View(dd)
View(mat)
options("contrasts")
model.matrix(~ a + b, dd)
out <- cbind(model.matrix( ~ 0 +  V1, mat), model.matrix( ~ 0 +  V2, mat), model.matrix( ~ 0 +  V3, mat))
View(out)
View(out)
View(mat)
View(mat)
mat <- do.call("rbind",strsplit(combo, "\\|"))
View(mat)
class(mat)
t(mat)
test <- t(mat)
View(test)
?t
mat <- as.data.frame(mat)
View(out)
library(dplyr)
mat %>%
group_by(mat$V1) %>%
summarise(count() = n())
mat %>%
group_by(mat$V1) %>%
summarise(count = n())
mat %>%
group_by(V1) %>%
summarise(count = n())
mat %>%
group_by(V1, V2) %>%
summarise(count = n())
